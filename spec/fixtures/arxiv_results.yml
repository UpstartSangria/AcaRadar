---
meta:
  query: all:Reinforcement Learning AND submittedDate:[201010020000 TO 202510020000]
  max_results: 50
  sort_by: submittedDate
  sort_order: ascending
  fetched_at: '2025-10-10T06:54:24Z'
total_results: 410910
entries:
- id: http://arxiv.org/abs/1010.0287v1
  title: |-
    Queue-Aware Distributive Resource Control for Delay-Sensitive Two-Hop
      MIMO Cooperative Systems
  summary: |-
    In this paper, we consider a queue-aware distributive resource control
    algorithm for two-hop MIMO cooperative systems. We shall illustrate that relay
    buffering is an effective way to reduce the intrinsic half-duplex penalty in
    cooperative systems. The complex interactions of the queues at the source node
    and the relays are modeled as an average-cost infinite horizon Markov Decision
    Process (MDP). The traditional approach solving this MDP problem involves
    centralized control with huge complexity. To obtain a distributive and low
    complexity solution, we introduce a linear structure which approximates the
    value function of the associated Bellman equation by the sum of per-node value
    functions. We derive a distributive two-stage two-winner auction-based control
    policy which is a function of the local CSI and local QSI only. Furthermore, to
    estimate the best fit approximation parameter, we propose a distributive online
    stochastic learning algorithm using stochastic approximation theory. Finally,
    we establish technical conditions for almost-sure convergence and show that
    under heavy traffic, the proposed low complexity distributive control is global
    optimal.
  published: '2010-10-02T03:57:46Z'
  updated: '2010-10-02T03:57:46Z'
  authors: &1
  - Rui Wang
  - Vincent K. N. Lau
  - Ying Cui
  categories:
  - cs.LG
  - MIMO, relay, queue-aware, distributive resource control
  primary_category: cs.LG
  links:
  - rel: related
    href: http://dx.doi.org/10.1109/TSP.2010.2086449
    title: doi
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0287v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0287v1
    title: pdf
- id: http://arxiv.org/abs/1010.0520v2
  title: Successive normalization of rectangular arrays
  summary: |-
    Standard statistical techniques often require transforming data to have mean
    $0$ and standard deviation $1$. Typically, this process of "standardization" or
    "normalization" is applied across subjects when each subject produces a single
    number. High throughput genomic and financial data often come as rectangular
    arrays where each coordinate in one direction concerns subjects who might have
    different status (case or control, say), and each coordinate in the other
    designates "outcome" for a specific feature, for example, "gene," "polymorphic
    site" or some aspect of financial profile. It may happen, when analyzing data
    that arrive as a rectangular array, that one requires BOTH the subjects and the
    features to be "on the same footing." Thus there may be a need to standardize
    across rows and columns of the rectangular matrix. There arises the question as
    to how to achieve this double normalization. We propose and investigate the
    convergence of what seems to us a natural approach to successive normalization
    which we learned from our colleague Bradley Efron. We also study the
    implementation of the method on simulated data and also on data that arose from
    scientific experimentation.
  published: '2010-10-04T09:48:16Z'
  updated: '2013-12-11T12:51:40Z'
  authors:
  - Richard A. Olshen
  - Bala Rajaratnam
  categories:
  - math.ST
  - stat.TH
  primary_category: math.ST
  links:
  - rel: related
    href: http://dx.doi.org/10.1214/09-AOS743
    title: doi
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0520v2
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0520v2
    title: pdf
- id: http://arxiv.org/abs/1010.0535v3
  title: |-
    Asymptotic Normality of Support Vector Machine Variants and Other
      Regularized Kernel Methods
  summary: |-
    In nonparametric classification and regression problems, regularized kernel
    methods, in particular support vector machines, attract much attention in
    theoretical and in applied statistics. In an abstract sense, regularized kernel
    methods (simply called SVMs here) can be seen as regularized M-estimators for a
    parameter in a (typically infinite dimensional) reproducing kernel Hilbert
    space. For smooth loss functions, it is shown that the difference between the
    estimator, i.e.\ the empirical SVM, and the theoretical SVM is asymptotically
    normal with rate $\sqrt{n}$. That is, the standardized difference converges
    weakly to a Gaussian process in the reproducing kernel Hilbert space. As common
    in real applications, the choice of the regularization parameter may depend on
    the data. The proof is done by an application of the functional delta-method
    and by showing that the SVM-functional is suitably Hadamard-differentiable.
  published: '2010-10-04T10:46:32Z'
  updated: '2011-04-12T07:08:05Z'
  authors:
  - Robert Hable
  categories:
  - stat.ML
  - 62G08, 62G20, 62M10
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0535v3
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0535v3
    title: pdf
- id: http://arxiv.org/abs/1010.0556v2
  title: Regularizers for Structured Sparsity
  summary: |-
    We study the problem of learning a sparse linear regression vector under
    additional conditions on the structure of its sparsity pattern. This problem is
    relevant in machine learning, statistics and signal processing. It is well
    known that a linear regression can benefit from knowledge that the underlying
    regression vector is sparse. The combinatorial problem of selecting the nonzero
    components of this vector can be "relaxed" by regularizing the squared error
    with a convex penalty function like the $\ell_1$ norm. However, in many
    applications, additional conditions on the structure of the regression vector
    and its sparsity pattern are available. Incorporating this information into the
    learning method may lead to a significant decrease of the estimation error. In
    this paper, we present a family of convex penalty functions, which encode prior
    knowledge on the structure of the vector formed by the absolute values of the
    regression coefficients. This family subsumes the $\ell_1$ norm and is flexible
    enough to include different models of sparsity patterns, which are of practical
    and theoretical importance. We establish the basic properties of these penalty
    functions and discuss some examples where they can be computed explicitly.
    Moreover, we present a convergent optimization algorithm for solving
    regularized least squares with these penalty functions. Numerical simulations
    highlight the benefit of structured sparsity and the advantage offered by our
    approach over the Lasso method and other related methods.
  published: '2010-10-04T12:04:44Z'
  updated: '2011-03-30T11:24:17Z'
  authors:
  - Charles A. Micchelli
  - Jean M. Morales
  - Massimiliano Pontil
  categories:
  - stat.ML
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0556v2
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0556v2
    title: pdf
- id: http://arxiv.org/abs/1010.0609v1
  title: Selfish Response to Epidemic Propagation
  summary: |-
    An epidemic spreading in a network calls for a decision on the part of the
    network members: They should decide whether to protect themselves or not. Their
    decision depends on the trade-off between their perceived risk of being
    infected and the cost of being protected. The network members can make
    decisions repeatedly, based on information that they receive about the changing
    infection level in the network.
      We study the equilibrium states reached by a network whose members increase
    (resp. decrease) their security deployment when learning that the network
    infection is widespread (resp. limited). Our main finding is that the
    equilibrium level of infection increases as the learning rate of the members
    increases. We confirm this result in three scenarios for the behavior of the
    members: strictly rational cost minimizers, not strictly rational, and strictly
    rational but split into two response classes. In the first two cases, we
    completely characterize the stability and the domains of attraction of the
    equilibrium points, even though the first case leads to a differential
    inclusion. We validate our conclusions with simulations on human mobility
    traces.
  published: '2010-10-04T14:51:58Z'
  updated: '2010-10-04T14:51:58Z'
  authors:
  - George Theodorakopoulos
  - Jean-Yves Le Boudec
  - John S. Baras
  categories:
  - cs.SY
  - cs.MA
  - nlin.AO
  primary_category: cs.SY
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0609v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0609v1
    title: pdf
- id: http://arxiv.org/abs/1010.0621v2
  title: Local Optimality of User Choices and Collaborative Competitive Filtering
  summary: |-
    While a user's preference is directly reflected in the interactive choice
    process between her and the recommender, this wealth of information was not
    fully exploited for learning recommender models. In particular, existing
    collaborative filtering (CF) approaches take into account only the binary
    events of user actions but totally disregard the contexts in which users'
    decisions are made. In this paper, we propose Collaborative Competitive
    Filtering (CCF), a framework for learning user preferences by modeling the
    choice process in recommender systems. CCF employs a multiplicative latent
    factor model to characterize the dyadic utility function. But unlike CF, CCF
    models the user behavior of choices by encoding a local competition effect. In
    this way, CCF allows us to leverage dyadic data that was previously lumped
    together with missing data in existing CF models. We present two formulations
    and an efficient large scale optimization algorithm. Experiments on three
    real-world recommendation data sets demonstrate that CCF significantly
    outperforms standard CF approaches in both offline and online evaluations.
  published: '2010-10-04T15:29:33Z'
  updated: '2011-02-25T21:37:16Z'
  authors:
  - Shuang Hong Yang
  categories:
  - stat.ML
  - cs.IR
  - cs.SI
  - stat.AP
  - I.2.6; H.1.1; H.3.3
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0621v2
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0621v2
    title: pdf
- id: http://arxiv.org/abs/1010.0703v2
  title: |-
    Implementing regularization implicitly via approximate eigenvector
      computation
  summary: |-
    Regularization is a powerful technique for extracting useful information from
    noisy data. Typically, it is implemented by adding some sort of norm constraint
    to an objective function and then exactly optimizing the modified objective
    function. This procedure often leads to optimization problems that are
    computationally more expensive than the original problem, a fact that is
    clearly problematic if one is interested in large-scale applications. On the
    other hand, a large body of empirical work has demonstrated that heuristics,
    and in some cases approximation algorithms, developed to speed up computations
    sometimes have the side-effect of performing regularization implicitly. Thus,
    we consider the question: What is the regularized optimization objective that
    an approximation algorithm is exactly optimizing?
      We address this question in the context of computing approximations to the
    smallest nontrivial eigenvector of a graph Laplacian; and we consider three
    random-walk-based procedures: one based on the heat kernel of the graph, one
    based on computing the the PageRank vector associated with the graph, and one
    based on a truncated lazy random walk. In each case, we provide a precise
    characterization of the manner in which the approximation method can be viewed
    as implicitly computing the exact solution to a regularized problem.
    Interestingly, the regularization is not on the usual vector form of the
    optimization problem, but instead it is on a related semidefinite program.
  published: '2010-10-04T20:49:15Z'
  updated: '2011-04-27T03:52:25Z'
  authors:
  - Michael W. Mahoney
  - Lorenzo Orecchia
  categories:
  - cs.DS
  - stat.CO
  - stat.ML
  primary_category: cs.DS
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0703v2
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0703v2
    title: pdf
- id: http://arxiv.org/abs/1010.0772v1
  title: A bagging SVM to learn from positive and unlabeled examples
  summary: |-
    We consider the problem of learning a binary classifier from a training set
    of positive and unlabeled examples, both in the inductive and in the
    transductive setting. This problem, often referred to as \emph{PU learning},
    differs from the standard supervised classification problem by the lack of
    negative examples in the training set. It corresponds to an ubiquitous
    situation in many applications such as information retrieval or gene ranking,
    when we have identified a set of data of interest sharing a particular
    property, and we wish to automatically retrieve additional data sharing the
    same property among a large and easily available pool of unlabeled data. We
    propose a conceptually simple method, akin to bagging, to approach both
    inductive and transductive PU learning problems, by converting them into series
    of supervised binary classification problems discriminating the known positive
    examples from random subsamples of the unlabeled set. We empirically
    demonstrate the relevance of the method on simulated and real data, where it
    performs at least as well as existing methods while being faster.
  published: '2010-10-05T06:03:09Z'
  updated: '2010-10-05T06:03:09Z'
  authors:
  - Fantine Mordelet
  - Jean-Philippe Vert
  categories:
  - stat.ML
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0772v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0772v1
    title: pdf
- id: http://arxiv.org/abs/1010.0789v2
  title: Estimation of low-rank tensors via convex optimization
  summary: |-
    In this paper, we propose three approaches for the estimation of the Tucker
    decomposition of multi-way arrays (tensors) from partial observations. All
    approaches are formulated as convex minimization problems. Therefore, the
    minimum is guaranteed to be unique. The proposed approaches can automatically
    estimate the number of factors (rank) through the optimization. Thus, there is
    no need to specify the rank beforehand. The key technique we employ is the
    trace norm regularization, which is a popular approach for the estimation of
    low-rank matrices. In addition, we propose a simple heuristic to improve the
    interpretability of the obtained factorization. The advantages and
    disadvantages of three proposed approaches are demonstrated through numerical
    experiments on both synthetic and real world datasets. We show that the
    proposed convex optimization based approaches are more accurate in predictive
    performance, faster, and more reliable in recovering a known multilinear
    structure than conventional approaches.
  published: '2010-10-05T08:00:33Z'
  updated: '2011-03-02T08:12:24Z'
  authors:
  - Ryota Tomioka
  - Kohei Hayashi
  - Hisashi Kashima
  categories:
  - stat.ML
  - math.NA
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.0789v2
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.0789v2
    title: pdf
- id: http://arxiv.org/abs/1010.1042v3
  title: Hidden Markov Models with Multiple Observation Processes
  summary: |-
    We consider a hidden Markov model with multiple observation processes, one of
    which is chosen at each point in time by a policy---a deterministic function of
    the information state---and attempt to determine which policy minimises the
    limiting expected entropy of the information state. Focusing on a special case,
    we prove analytically that the information state always converges in
    distribution, and derive a formula for the limiting entropy which can be used
    for calculations with high precision. Using this fomula, we find
    computationally that the optimal policy is always a threshold policy, allowing
    it to be easily found. We also find that the greedy policy is almost optimal.
  published: '2010-10-06T00:36:04Z'
  updated: '2011-05-05T08:34:07Z'
  authors:
  - James Y. Zhao
  categories:
  - math.PR
  - cs.IT
  - cs.LG
  - math.IT
  - 90C40
  primary_category: math.PR
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1042v3
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1042v3
    title: pdf
- id: http://arxiv.org/abs/1010.1409v1
  title: |-
    A sparse regulatory network of copy-number driven expression reveals
      putative breast cancer oncogenes
  summary: |-
    The influence of DNA cis-regulatory elements on a gene's expression has been
    intensively studied. However, little is known about expressions driven by
    trans-acting DNA hotspots. DNA hotspots harboring copy number aberrations are
    recognized to be important in cancer as they influence multiple genes on a
    global scale. The challenge in detecting trans-effects is mainly due to the
    computational difficulty in detecting weak and sparse trans-acting signals
    amidst co-occuring passenger events. We propose an integrative approach to
    learn a sparse interaction network of DNA copy-number regions with their
    downstream targets in a breast cancer dataset. Information from this network
    helps distinguish copy-number driven from copy-number independent expression
    changes on a global scale. Our result further delineates cis- and trans-effects
    in a breast cancer dataset, for which important oncogenes such as ESR1 and
    ERBB2 appear to be highly copy-number dependent. Further, our model is shown to
    be efficient and in terms of goodness of fit no worse than other state-of the
    art predictors and network reconstruction models using both simulated and real
    data.
  published: '2010-10-07T12:12:13Z'
  updated: '2010-10-07T12:12:13Z'
  authors:
  - Yinyin Yuan
  - Christina Curtis
  - Carlos Caldas
  - Florian Markowetz
  categories:
  - q-bio.MN
  - stat.AP
  primary_category: q-bio.MN
  links:
  - rel: related
    href: http://dx.doi.org/10.1109/BIBM.2010.5706612
    title: doi
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1409v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1409v1
    title: pdf
- id: http://arxiv.org/abs/1010.1437v1
  title: Mixed-Membership Stochastic Block-Models for Transactional Networks
  summary: |-
    Transactional network data can be thought of as a list of one-to-many
    communications(e.g., email) between nodes in a social network. Most social
    network models convert this type of data into binary relations between pairs of
    nodes. We develop a latent mixed membership model capable of modeling richer
    forms of transactional network data, including relations between more than two
    nodes. The model can cluster nodes and predict transactions. The block-model
    nature of the model implies that groups can be characterized in very general
    ways. This flexible notion of group structure enables discovery of rich
    structure in transactional networks. Estimation and inference are accomplished
    via a variational EM algorithm. Simulations indicate that the learning
    algorithm can recover the correct generative model. Interesting structure is
    discovered in the Enron email dataset and another dataset extracted from the
    Reddit website. Analysis of the Reddit data is facilitated by a novel
    performance measure for comparing two soft clusterings. The new model is
    superior at discovering mixed membership in groups and in predicting
    transactions.
  published: '2010-10-07T14:16:38Z'
  updated: '2010-10-07T14:16:38Z'
  authors:
  - Mahdi Shafiei
  - Hugh Chipman
  categories:
  - stat.ML
  - cs.AI
  - cs.SI
  - stat.AP
  - stat.ME
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1437v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1437v1
    title: pdf
- id: http://arxiv.org/abs/1010.1499v3
  title: |-
    Completely Stale Transmitter Channel State Information is Still Very
      Useful
  summary: |-
    Transmitter channel state information (CSIT) is crucial for the multiplexing
    gains offered by advanced interference management techniques such as multiuser
    MIMO and interference alignment. Such CSIT is usually obtained by feedback from
    the receivers, but the feedback is subject to delays. The usual approach is to
    use the fed back information to predict the current channel state and then
    apply a scheme designed assuming perfect CSIT. When the feedback delay is large
    compared to the channel coherence time, such a prediction approach completely
    fails to achieve any multiplexing gain. In this paper, we show that even in
    this case, the completely stale CSI is still very useful. More concretely, we
    show that in a MIMO broadcast channel with $K$ transmit antennas and $K$
    receivers each with 1 receive antenna, $\frac{K}{1+1/2+ ...+ \frac{1}{K}} (> 1)
    $ degrees of freedom is achievable even when the fed back channel state is
    completely independent of the current channel state. Moreover, we establish
    that if all receivers have independent and identically distributed channels,
    then this is the optimal number of degrees of freedom achievable. In the
    optimal scheme, the transmitter uses the fed back CSI to learn the side
    information that the receivers receive from previous transmissions rather than
    to predict the current channel state. Our result can be viewed as the first
    example of feedback providing a degree-of-freedom gain in memoryless channels.
  published: '2010-10-07T18:00:46Z'
  updated: '2012-06-29T16:38:17Z'
  authors:
  - Mohammad Ali Maddah-Ali
  - David Tse
  categories:
  - cs.IT
  - math.IT
  primary_category: cs.IT
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1499v3
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1499v3
    title: pdf
- id: http://arxiv.org/abs/1010.1526v6
  title: |-
    Time Series Classification by Class-Specific Mahalanobis Distance
      Measures
  summary: |-
    To classify time series by nearest neighbors, we need to specify or learn one
    or several distance measures. We consider variations of the Mahalanobis
    distance measures which rely on the inverse covariance matrix of the data.
    Unfortunately --- for time series data --- the covariance matrix has often low
    rank. To alleviate this problem we can either use a pseudoinverse, covariance
    shrinking or limit the matrix to its diagonal. We review these alternatives and
    benchmark them against competitive methods such as the related Large Margin
    Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW)
    distance. As we expected, we find that the DTW is superior, but the Mahalanobis
    distance measures are one to two orders of magnitude faster. To get best
    results with Mahalanobis distance measures, we recommend learning one distance
    measure per class using either covariance shrinking or the diagonal approach.
  published: '2010-10-07T19:48:23Z'
  updated: '2012-07-02T20:57:01Z'
  authors:
  - Zoltán Prekopcsák
  - Daniel Lemire
  categories:
  - cs.LG
  primary_category: cs.LG
  links:
  - rel: related
    href: http://dx.doi.org/10.1007/s11634-012-0110-6
    title: doi
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1526v6
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1526v6
    title: pdf
- id: http://arxiv.org/abs/1010.1609v1
  title: Algorithmic and Statistical Perspectives on Large-Scale Data Analysis
  summary: |-
    In recent years, ideas from statistics and scientific computing have begun to
    interact in increasingly sophisticated and fruitful ways with ideas from
    computer science and the theory of algorithms to aid in the development of
    improved worst-case algorithms that are useful for large-scale scientific and
    Internet data analysis problems. In this chapter, I will describe two recent
    examples---one having to do with selecting good columns or features from a (DNA
    Single Nucleotide Polymorphism) data matrix, and the other having to do with
    selecting good clusters or communities from a data graph (representing a social
    or information network)---that drew on ideas from both areas and that may serve
    as a model for exploiting complementary algorithmic and statistical
    perspectives in order to solve applied large-scale data analysis problems.
  published: '2010-10-08T07:02:11Z'
  updated: '2010-10-08T07:02:11Z'
  authors:
  - Michael W. Mahoney
  categories:
  - cs.DS
  - stat.CO
  - stat.ML
  primary_category: cs.DS
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1609v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1609v1
    title: pdf
- id: http://arxiv.org/abs/1010.1763v3
  title: Algorithms for nonnegative matrix factorization with the beta-divergence
  summary: |-
    This paper describes algorithms for nonnegative matrix factorization (NMF)
    with the beta-divergence (beta-NMF). The beta-divergence is a family of cost
    functions parametrized by a single shape parameter beta that takes the
    Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito
    divergence as special cases (beta = 2,1,0, respectively). The proposed
    algorithms are based on a surrogate auxiliary function (a local majorization of
    the criterion function). We first describe a majorization-minimization (MM)
    algorithm that leads to multiplicative updates, which differ from standard
    heuristic multiplicative updates by a beta-dependent power exponent. The
    monotonicity of the heuristic algorithm can however be proven for beta in (0,1)
    using the proposed auxiliary function. Then we introduce the concept of
    majorization-equalization (ME) algorithm which produces updates that move along
    constant level sets of the auxiliary function and lead to larger steps than MM.
    Simulations on synthetic and real data illustrate the faster convergence of the
    ME approach. The paper also describes how the proposed algorithms can be
    adapted to two common variants of NMF : penalized NMF (i.e., when a penalty
    function of the factors is added to the criterion function) and convex-NMF
    (when the dictionary is assumed to belong to a known subspace).
  published: '2010-10-08T18:53:27Z'
  updated: '2011-03-08T12:56:39Z'
  authors:
  - Cédric Févotte
  - Jérôme Idier
  categories:
  - cs.LG
  primary_category: cs.LG
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1763v3
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1763v3
    title: pdf
- id: http://arxiv.org/abs/1010.1868v1
  title: |-
    Infinite Hierarchical MMSB Model for Nested Communities/Groups in Social
      Networks
  summary: |-
    Actors in realistic social networks play not one but a number of diverse
    roles depending on whom they interact with, and a large number of such
    role-specific interactions collectively determine social communities and their
    organizations. Methods for analyzing social networks should capture these
    multi-faceted role-specific interactions, and, more interestingly, discover the
    latent organization or hierarchy of social communities. We propose a
    hierarchical Mixed Membership Stochastic Blockmodel to model the generation of
    hierarchies in social communities, selective membership of actors to subsets of
    these communities, and the resultant networks due to within- and
    cross-community interactions. Furthermore, to automatically discover these
    latent structures from social networks, we develop a Gibbs sampling algorithm
    for our model. We conduct extensive validation of our model using synthetic
    networks, and demonstrate the utility of our model in real-world datasets such
    as predator-prey networks and citation networks.
  published: '2010-10-09T19:43:56Z'
  updated: '2010-10-09T19:43:56Z'
  authors:
  - Qirong Ho
  - Ankur P. Parikh
  - Le Song
  - Eric P. Xing
  categories:
  - stat.ML
  - stat.ME
  primary_category: stat.ML
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1868v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1868v1
    title: pdf
- id: http://arxiv.org/abs/1010.1888v1
  title: |-
    Multi-Objective Genetic Programming Projection Pursuit for Exploratory
      Data Modeling
  summary: |-
    For classification problems, feature extraction is a crucial process which
    aims to find a suitable data representation that increases the performance of
    the machine learning algorithm. According to the curse of dimensionality
    theorem, the number of samples needed for a classification task increases
    exponentially as the number of dimensions (variables, features) increases. On
    the other hand, it is costly to collect, store and process data. Moreover,
    irrelevant and redundant features might hinder classifier performance. In
    exploratory analysis settings, high dimensionality prevents the users from
    exploring the data visually. Feature extraction is a two-step process: feature
    construction and feature selection. Feature construction creates new features
    based on the original features and feature selection is the process of
    selecting the best features as in filter, wrapper and embedded methods.
      In this work, we focus on feature construction methods that aim to decrease
    data dimensionality for visualization tasks. Various linear (such as principal
    components analysis (PCA), multiple discriminants analysis (MDA), exploratory
    projection pursuit) and non-linear (such as multidimensional scaling (MDS),
    manifold learning, kernel PCA/LDA, evolutionary constructive induction)
    techniques have been proposed for dimensionality reduction. Our algorithm is an
    adaptive feature extraction method which consists of evolutionary constructive
    induction for feature construction and a hybrid filter/wrapper method for
    feature selection.
  published: '2010-10-10T02:34:22Z'
  updated: '2010-10-10T02:34:22Z'
  authors:
  - Ilknur Icke
  - Andrew Rosenberg
  categories:
  - cs.LG
  - cs.NE
  primary_category: cs.LG
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.1888v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.1888v1
    title: pdf
- id: http://arxiv.org/abs/1010.2102v1
  title: |-
    Hierarchical Multiclass Decompositions with Application to Authorship
      Determination
  summary: |-
    This paper is mainly concerned with the question of how to decompose
    multiclass classification problems into binary subproblems. We extend known
    Jensen-Shannon bounds on the Bayes risk of binary problems to hierarchical
    multiclass problems and use these bounds to develop a heuristic procedure for
    constructing hierarchical multiclass decomposition for multinomials. We test
    our method and compare it to the well known "all-pairs" decomposition. Our
    tests are performed using a new authorship determination benchmark test of
    machine learning authors. The new method consistently outperforms the all-pairs
    decomposition when the number of classes is small and breaks even on larger
    multiclass problems. Using both methods, the classification accuracy we
    achieve, using an SVM over a feature set consisting of both high frequency
    single tokens and high frequency token-pairs, appears to be exceptionally high
    compared to known results in authorship determination.
  published: '2010-10-11T13:41:21Z'
  updated: '2010-10-11T13:41:21Z'
  authors:
  - Ran El-Yaniv
  - Noam Etzion-Rosenberg
  categories:
  - cs.AI
  primary_category: cs.AI
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2102v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2102v1
    title: pdf
- id: http://arxiv.org/abs/1010.2138v1
  title: |-
    Combiner suivi de l'activite? et partage d'expériences en
      apprentissage par projet pour les acteurs tuteurs et apprenants
  summary: |-
    Our work aims to study tools offered to students and tutors involved in
    face-to-face or blended project- based learning activities. Project-based
    learning is often applied in the case of complex learning (i.e. which aims at
    making learners acquire various linked skills or develop their behaviours). In
    comparison to traditional learning, this type of learning relies on
    co-development, collective responsibility and co-operation. Learners are the
    principal actors of their learning. These trainings rest on rich and complex
    organizations, particularly for tutors, and it is difficult to apply innovative
    educational strategies. Our aim, in a bottom-up approach, is (1) to observe,
    according to Knowledge Management methods, a course characterized by these
    three criteria. The observed course concerns project management learning. Its
    observation allows us (2) to highlight and to analyze the problems encountered
    by the actors (students, tutors, designers) and (3) to propose tools to solve
    or improve them. We particularly study the relevance and the limits of the
    existing monitoring and experience sharing tools. We finally propose a result
    in the form of the tool MEShaT (Monitoring and Experience Sharing Tool) and end
    on the perspectives offered by these researches.
  published: '2010-10-11T15:34:46Z'
  updated: '2010-10-11T15:34:46Z'
  authors:
  - Christine Michel
  - Elise Lavoué
  categories:
  - cs.CY
  primary_category: cs.CY
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2138v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2138v1
    title: pdf
- id: http://arxiv.org/abs/1010.2285v3
  title: |-
    Information-based complexity, feedback and dynamics in convex
      programming
  summary: |-
    We study the intrinsic limitations of sequential convex optimization through
    the lens of feedback information theory. In the oracle model of optimization,
    an algorithm queries an {\em oracle} for noisy information about the unknown
    objective function, and the goal is to (approximately) minimize every function
    in a given class using as few queries as possible. We show that, in order for a
    function to be optimized, the algorithm must be able to accumulate enough
    information about the objective. This, in turn, puts limits on the speed of
    optimization under specific assumptions on the oracle and the type of feedback.
    Our techniques are akin to the ones used in statistical literature to obtain
    minimax lower bounds on the risks of estimation procedures; the notable
    difference is that, unlike in the case of i.i.d. data, a sequential
    optimization algorithm can gather observations in a {\em controlled} manner, so
    that the amount of information at each step is allowed to change in time. In
    particular, we show that optimization algorithms often obey the law of
    diminishing returns: the signal-to-noise ratio drops as the optimization
    algorithm approaches the optimum. To underscore the generality of the tools, we
    use our approach to derive fundamental lower bounds for a certain active
    learning problem. Overall, the present work connects the intuitive notions of
    information in optimization, experimental design, estimation, and active
    learning to the quantitative notion of Shannon information.
  published: '2010-10-12T02:19:43Z'
  updated: '2011-09-09T15:57:54Z'
  authors:
  - Maxim Raginsky
  - Alexander Rakhlin
  categories:
  - cs.IT
  - cs.SY
  - math.IT
  - math.OC
  primary_category: cs.IT
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2285v3
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2285v3
    title: pdf
- id: http://arxiv.org/abs/1010.2286v1
  title: |-
    Divergence-based characterization of fundamental limitations of adaptive
      dynamical systems
  summary: |-
    Adaptive dynamical systems arise in a multitude of contexts, e.g.,
    optimization, control, communications, signal processing, and machine learning.
    A precise characterization of their fundamental limitations is therefore of
    paramount importance. In this paper, we consider the general problem of
    adaptively controlling and/or identifying a stochastic dynamical system, where
    our {\em a priori} knowledge allows us to place the system in a subset of a
    metric space (the uncertainty set). We present an information-theoretic
    meta-theorem that captures the trade-off between the metric complexity (or
    richness) of the uncertainty set, the amount of information acquired online in
    the process of controlling and observing the system, and the residual
    uncertainty remaining after the observations have been collected. Following the
    approach of Zames, we quantify {\em a priori} information by the Kolmogorov
    (metric) entropy of the uncertainty set, while the information acquired online
    is expressed as a sum of information divergences. The general theory is used to
    derive new minimax lower bounds on the metric identification error, as well as
    to give a simple derivation of the minimum time needed to stabilize an
    uncertain stochastic linear system.
  published: '2010-10-12T02:27:59Z'
  updated: '2010-10-12T02:27:59Z'
  authors:
  - Maxim Raginsky
  categories:
  - cs.IT
  - math.IT
  - math.OC
  primary_category: cs.IT
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2286v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2286v1
    title: pdf
- id: http://arxiv.org/abs/1010.2354v1
  title: Predicting Coding Effort in Projects Containing XML Code
  summary: |-
    This paper studies the problem of predicting the coding effort for a
    subsequent year of development by analysing metrics extracted from project
    repositories, with an emphasis on projects containing XML code. The study
    considers thirteen open source projects and applies machine learning algorithms
    to generate models to predict one-year coding effort, measured in terms of
    lines of code added, modified and deleted. Both organisational and code metrics
    associated to revisions are taken into account. The results show that coding
    effort is highly determined by the expertise of developers while source code
    metrics have little effect on improving the accuracy of estimations of coding
    effort. The study also shows that models trained on one project are unreliable
    at estimating effort in other projects.
  published: '2010-10-12T11:30:47Z'
  updated: '2010-10-12T11:30:47Z'
  authors:
  - Siim Karus
  - Marlon Dumas
  categories:
  - cs.SE
  - D.2.7; D.2.8; D.2.9
  primary_category: cs.SE
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2354v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2354v1
    title: pdf
- id: http://arxiv.org/abs/1010.2384v1
  title: Learning Taxonomy for Text Segmentation by Formal Concept Analysis
  summary: |-
    In this paper the problems of deriving a taxonomy from a text and
    concept-oriented text segmentation are approached. Formal Concept Analysis
    (FCA) method is applied to solve both of these linguistic problems. The
    proposed segmentation method offers a conceptual view for text segmentation,
    using a context-driven clustering of sentences. The Concept-oriented Clustering
    Segmentation algorithm (COCS) is based on k-means linear clustering of the
    sentences. Experimental results obtained using COCS algorithm are presented.
  published: '2010-10-12T13:20:30Z'
  updated: '2010-10-12T13:20:30Z'
  authors:
  - Mihaiela Lupea
  - Doina Tatar
  - Zsuzsana Marian
  categories:
  - cs.CL
  - 68T50, 03H65
  primary_category: cs.CL
  links:
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2384v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2384v1
    title: pdf
- id: http://arxiv.org/abs/1010.2443v1
  title: Leading twist shadowing, black disk regime and forward hadron production
  summary: |-
    We review theory of the leading twist nuclear shadowing, and describe
    phenomenon of post-selection suppression of leading parton spectrum (effective
    fractional energy losses) in the proximity of the black disk regime. We argue
    that $2 \to 2$ mechanism dominates in the inclusive leading pion production in
    d-Au collisions and explain that the post-selection naturally explains both the
    magnitude of the suppression of the forward pion production in d-Au collisions
    and the pattern of the forward - central correlations. At the same time this
    pattern of correlations rules out $2\to 1$ mechanism as the main source of the
    inclusive leading pion yield. It is demonstrated that the mechanism of the
    double parton interactions gives an important contribution to the production of
    two leading pions in $pp$ scattering opening a new way to study correlations of
    leading quarks in the nucleon. The same mechanism is enhanced in $dAu \to
    \pi^0\pi^0 +X$ collisions and explains the dominance of $\Delta\phi$
    independent component and suppression of the away side peak.
  published: '2010-10-12T17:02:03Z'
  updated: '2010-10-12T17:02:03Z'
  authors:
  - Mark Strikman
  categories:
  - nucl-th
  primary_category: nucl-th
  links:
  - rel: related
    href: http://dx.doi.org/10.1016/j.nuclphysa.2010.10.003
    title: doi
  - rel: alternate
    type: text/html
    href: http://arxiv.org/abs/1010.2443v1
  - rel: related
    type: application/pdf
    href: http://arxiv.org/pdf/1010.2443v1
    title: pdf
titles:
- |-
  Queue-Aware Distributive Resource Control for Delay-Sensitive Two-Hop
    MIMO Cooperative Systems
- Successive normalization of rectangular arrays
- |-
  Asymptotic Normality of Support Vector Machine Variants and Other
    Regularized Kernel Methods
- Regularizers for Structured Sparsity
- Selfish Response to Epidemic Propagation
- Local Optimality of User Choices and Collaborative Competitive Filtering
- |-
  Implementing regularization implicitly via approximate eigenvector
    computation
- A bagging SVM to learn from positive and unlabeled examples
- Estimation of low-rank tensors via convex optimization
- Hidden Markov Models with Multiple Observation Processes
- |-
  A sparse regulatory network of copy-number driven expression reveals
    putative breast cancer oncogenes
- Mixed-Membership Stochastic Block-Models for Transactional Networks
- |-
  Completely Stale Transmitter Channel State Information is Still Very
    Useful
- |-
  Time Series Classification by Class-Specific Mahalanobis Distance
    Measures
- Algorithmic and Statistical Perspectives on Large-Scale Data Analysis
- Algorithms for nonnegative matrix factorization with the beta-divergence
- |-
  Infinite Hierarchical MMSB Model for Nested Communities/Groups in Social
    Networks
- |-
  Multi-Objective Genetic Programming Projection Pursuit for Exploratory
    Data Modeling
- |-
  Hierarchical Multiclass Decompositions with Application to Authorship
    Determination
- |-
  Combiner suivi de l'activite? et partage d'expériences en
    apprentissage par projet pour les acteurs tuteurs et apprenants
- |-
  Information-based complexity, feedback and dynamics in convex
    programming
- |-
  Divergence-based characterization of fundamental limitations of adaptive
    dynamical systems
- Predicting Coding Effort in Projects Containing XML Code
- Learning Taxonomy for Text Segmentation by Formal Concept Analysis
- Leading twist shadowing, black disk regime and forward hadron production
first_entry_authors: *1
